\documentclass[notitlepage]{revtex4-1}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[margin=5pt]{subfig}
\usepackage[usenames]{color}
% \usepackage{xspace}
\definecolor{darkgreen}{rgb}{0.00,0.50,0.25}
\definecolor{darkblue}{rgb}{0.00,0.00,0.67}
\newcommand{\figref}[1]{Fig.~\ref{#1}}
\usepackage[breaklinks,pdftitle={ZeroDB whitepaper}, pdfauthor={Michael Egorov},colorlinks,urlcolor=blue,citecolor=darkgreen,linkcolor=darkblue]{hyperref}
\usepackage[usenames]{color}
\graphicspath{{pdf/}}

\begin{document}

\title{ZeroDB whitepaper}

\author{M. Egorov}
\email{michael@zerodb.io}
\author{M. Wilkison}
\email{maclane@zerodb.io}
\affiliation{ZeroDB, Inc.}

\begin{abstract}
ZeroDB is an end-to-end encrypted database that enables clients to operate on (search, sort, query, and share) encrypted data without exposing encryption keys or cleartext data to the database server.
The familiar client-server architecture remains, but query logic and encryption keys are pushed client-side.
Since the server has no insight into the nature of the data, the risk of a server-side data breach is eliminated.
Even if adversaries successfully infiltrate the server, they will not have access to the cleartext data.

ZeroDB provides end-to-end encryption while maintaining much of the functionality expected of a modern database, such as full-text search, sort, and range queries.
Additionally, ZeroDB uses proxy re-encryption and/or delta key technology to enable secure, granular sharing of encrypted data without exposing keys to the server and without sharing the same encryption key between all users of the database.
\end{abstract}

\date{\today}
\maketitle

\section{Introduction}

Given the cost, performance, and scalability benefits, outsourcing of on-premise infrastructure to cloud environments is accelerating.
However, concerns over data security and the frequency of high-profile data breaches hinder cloud adoption in highly-regulated industries, such as financial services.
While strong encryption can alleviate many of these concerns by guaranteeing data confidentiality, once data is encrypted, it is no longer usable.
Several strategies for computing on encrypted data have recently emerged that seek to provide the benefits of encryption while preserving the functionality of encryted data.

These techniques typically fall into two broad categories: (1) homomorphic encryption schemes that enable server-side computation directly over ciphertext and (2) trusted modules that are assumed to be inaccessible by adversaries\cite{tutorial}. These two approaches can be further deconstructed into (1) fully homomorphic encryption (FHE) and partially homomorphic encryption (PHE) and (2) client-end solutions and in-cloud solutions, resulting in the following taxonomy:

[Insert taxonomy of approaches here]

Review of existing systems needed:~\cite{cipherbase}~\cite{cryptdb}~\cite{gentry}~\cite{smart}


\section{Query protocol}

The basis of the end-to-end encrypted query protocol is as follows.
The client interacts with the server during the execution of a query over a series of round trips.
An encrypted index is stored on the server as a B-Tree, and the client traverses this index remotely to retrieve the necessary encrypted records.
The index consists of buckets which are encrypted before being uploaded to the server and only decrypted client-side.

\subsection{Equality query (using example of keyword search)}
\begin{figure}
	\begin{center}
        \subfloat[Encrypted index traversal example (simple keyword search)]{\label{fig:tree-traversal}\includegraphics[width=0.47\columnwidth]{btree-traverse.pdf}}
        \qquad
        \subfloat[Sequence of client requests for traversal of the example index]{\label{fig:communication}\includegraphics[width=0.47\columnwidth]{protocol.pdf}}
	\end{center}
    \caption{Search protocol for equality query using an example of keyword search}
	\label{fig:btree-protocol}
\end{figure}

In ZeroDB, indexes are structured as B-Trees~\figref{fig:tree-traversal}.
A B-Tree consists of buckets, each of which can be either a root, branch, or leaf node.
The leaf nodes of a tree point to the actual objects being stored.
Thus, searching the database is a simple tree traversal.

In order to make the database end-to-end encrypted yet still capable of performing queries, the client encrypts the buckets (at the time of creation or modification).
The server, which stores the buckets, never knows the encryption key used.
The objects referenced by the leaf nodes of the B-Tree indexes are also encrypted client-side.
As a result, the server does not know how individual objects are organized within the B-Tree or whether they even belong to an index at all.
Since ZeroDB is encryption agnostic, probabilistic encryption can be used so that the server cannot even compare objects for equality.

When a client performs a query, it asks the server to return buckets of the tree as it traverses the index remotely~\figref{fig:communication}.
Buckets can be cached client-side so that subsequent queries do not make unnecessary network calls.

The server is responsible for data replication, multi-version concurrency, object locking, user authentication, quotas, etc.
The client performs encryption/decryption and query logic.

\subsection{Range queries}

\begin{figure}
	\begin{center}
        \subfloat[Query searching for objects with a property $16 \le weight$, $limit=4$ (takes 4 requests w/o cache)]{\label{fig:range-query-iter}\includegraphics[width=0.47\columnwidth]{range-query-iter.pdf}}
        \qquad
        \subfloat[Query which fetches \emph{all} objects  with $16 \le weight \le 27$ (takes 3 requests w/o cache)]{\label{fig:range-query-star}\includegraphics[width=0.47\columnwidth]{range-query-star.pdf}}
	\end{center}
	\caption{Range queries}
	\label{fig:range-query}
\end{figure}

When data are organized in B-Trees, doing range queries is easy.
Take an example having records \emph{Record} with an integer property \emph{weight}.
Data pointers to \emph{Record} objects are placed in a B-Tree sorted by \emph{weight}.

Two different types of range queries could be performed.
One is when we want a small subset of data in the beginning of the range (limit query).
In this case, we find a pointer to the beginning of the range~\figref{fig:range-query-iter}, then incrementally fetch subsequent buckets if the range occupies more than one, then bulk-fetch objects themselves by their pointers.

The other case is when we want to get \emph{all} objects in the range (select *).
We download subsection of B-Tree matching the range query level-by-level in logarithmic number of steps in this case~\figref{fig:range-query-star}.
To do that, we start with the root bucket.
Then, we download all the child buckets which match the range at once.
Then all children of those buckets, repeating until we get to the leaf nodes.
After that we (optionally) bulk-fetch all the objects which match the range query at once.
This is done in a way similar to prefetching all trees (Section~\ref{sec:bulk-fetching}).
Selecting all objects in a range reveals the approximate number of objects in this range to the server (the range and field names remain secret).

\subsection{Complex queries (multi-keyword search, multiple conditions)}

So far we described simple queries.
But queries may contain multiple conditions at the same time.
Depending on number of objects matching each condition and desired security properties we can use different approaches.
Making a query with ``or'' condition requires simply zip-joining two sorted datasets, so we do not consider that to be problematic.
Instead, let us consider a query where we select objects matching the condition $(v_1 = a) \,\&\, (v_2 = b)$ and ordered by $v_3$.

\subsubsection{Prefetch approach}
\label{sec:prefetch}
%    a) var1: BTree(var1 -> TreeSet(ids))
%       var2: BTree(var2 -> TreeSet(ids))
%       Take shorter treeset (say, for var1), fetch all ids
%       Traverse treeset for var2 in parallel to find all the ids from 1st treeset
%       Find order by traversing reverse index var3: BTree(id -> var3)
%
%       Example: any typical highly selective query
%       Pros: easy to reorder afterwards
%       Cons: very strong hint about how large the dataset is (unless reading more data than actually need), bad performance if words have ~same weights

In case the number of items with $v_1=a$ is small, it could be worthwhile to download the entire subset of object IDs matching this condition.
This could be practical in a multi-keyword search when one word is rare.

We can use the following indexes for this kind of query:
\begin{align*}
    & \mbox{BTree}(v_1 \rightarrow \mbox{TreeSet}(ids)),\\
    & \mbox{BTree}(v_2 \rightarrow \mbox{TreeSet}(ids)),\\
    & \mbox{BTree}(id \rightarrow v_3).
\end{align*}

First, we estimate which condition has the smallest number of matching elements.
We can do so by fetching ``contours'' of the B-Trees (corresponding to smallest and largest elements of the range respectively) in order to determine the height of the tree and approximate distance between the smallest and largest elements, knowing average size of a bucket, and it ``costs'' $H$ requests between client and server.

We prefetch a TreeSet for the most ``lightweight'' condition (Sec.~\ref{sec:bulk-fetching}).
Then we bulk-search~(Sec.~\ref{sec:parallel-traversal}) these IDs in the larger TreeSet (and if we do not find an ID in the leaf node, we drop it).

After that, we want to sort the small subset of fetched IDs by $v_3$.
We do a parallel traversal of the third B-Tree and find which value of $v_3$ corresponds to each ID.

This approach reveals the number of elements matching condition $v_1=a$ to the server, although the condition itself remains unknown.

\subsubsection{Preorder approach}

The prefetch approach could be slow and reveal too much information to the server.
So, whenever possible, values are pre-ordered in the index.
It works in the following way.

The composite index to make a query selecting objects matching the condition $(v_1 = a) \,\&\, (v_2 = b)$ and ordered by $v_3$ is:
\begin{equation*}
    \mbox{BTree}((v_1, v_2) \rightarrow \mbox{BTree}(v_3 \rightarrow \mbox{TreeSet}(ids))).
\end{equation*}
For this query, we find a B-Tree (or multiple B-Trees) corresponding to the conditions and lazily traverse them in order of $v_3$.
This makes limit orders much more performant and does not leak information about possible dataset size to the server.

\subsubsection{Doing set intersection server-side}
\label{sec:server-intersection}

\begin{figure}
	\begin{center}
        \subfloat[Branch node]{\label{fig:keytree-branch-node}\includegraphics[width=0.47\columnwidth]{keytree-branch-bucket.pdf}}
        \qquad
        \subfloat[Leaf node]{\label{fig:keytree-leaf-node}\includegraphics[width=0.47\columnwidth]{keytree-leaf-bucket.pdf}}\\
        \subfloat[Hierarchy of keys (same as hierarchy of buckets)]{\label{fig:keytree}\includegraphics[width=0.7\columnwidth]{keytree.pdf}}
	\end{center}
    \caption{B-Tree structure which supports server-side set operations.
        In order for server to do these operations, the client allows it to see deterministically encrypted object IDs $D(id_i)$.
        The server can count the number of occurrences of each $D(id_i)$, and those values which are repeated the same number of times as the number of conditions in the ``and'' query are the ones the client wants to be returned.
    }
	\label{fig:server-side-sets}
\end{figure}

The performance bottleneck of many complex queries (such as multi-keyword search or queries with a product of multiple conditions) is set intersections.
For multi-keyword search for example, doing set intersections server-side would help significantly when each keyword has too many pages associated with it (which is possible for extremely large datasets).

Let us consider a query where $v_1=a$ and $v_2=b$.
In order to do server-side set intersection, we reveal deterministically encrypted IDs $D(id_i)$ of all objects matching each of these conditions to the server~(\figref{fig:server-side-sets}).
The server can see the frequency of each ID, which is equal to the number of conditions satisfied for this ID.
The server sends those deterministically encrypted IDs which satisfy all conditions ($2$ conditions in our example) to the client.

In order to do that, buckets are split into two sections~(\figref{fig:server-side-sets}~\subref{fig:keytree-branch-node},~\subref{fig:keytree-leaf-node}).
The first (top) section is encrypted with key $k^a$, which is known to the client but never known to the server.
The second (bottom) section is encrypted with a key $k^b_{*i}$.
Keys from $k^b$ family are hierarchical: child-keys $k^b_{*i}$ are derived from a key$k^b_*$  which can decrypt the bottom section of a node as well as the bottom sections of all its child nodes~(\figref{fig:keytree}).
The keys can be derived as $k^b_{*i} = \mbox{SHA256}(k^b_* + i)$, for example.

The client traverses the B-Tree along the ``contour'' of the range (red at~\figref{fig:keytree}).
It provides keys $k^b_i$ of buckets which are fully within the range and in-range deterministically encrypted object IDs $D(id_i)$ to the server.
The server, using this information, is able to determine all children of those buckets and the $D(id_i)$ associated with them.
After adding the $D(id_i)$ given by the client, the server can count how many times each value is repeated.
Values which are repeated the same number of times as the number of conditions in the ``and'' query are returned to the client.

In this case, the server can determine the total number of records matching any of the given conditions $n_1$ (the conditions themselves are unknown to the server),
the number of records matching exactly two of the conditions $n_2$,
etc. until the maximum number of conditions.
The server also learns ``parent-child'' relationships between some of buckets.
The server does not learn the order of the objects,
it does not learn which conditions the client is applying.

\subsubsection{Related objects}

ZeroDB is based on ZODB, which uses object references instead of joins~\cite{zodb-references},
similar to mongo's database references~\cite{mongo-db-references}.
In order to keep everything consistent, ZODB fires events at commit time~\cite{zope-events} to keep back-references and indexes up to date.
In ZeroDB (as in ZODB), we can create indexes on attributes of related objects, much as we create normal indexes.

\subsection{Optimizations specific for remote client}

In most cases, all the query logic in ZeroDB happens client-side, e.g. client and storage are separated by a network channel with high latency.
Therefore, we use two primitives specific for this architecture.

\begin{figure}
	\begin{center}
        \subfloat[When a tree (or sub-tree) is small, it can be fully pre-fetched to the client]{\label{fig:fetch-tree}\includegraphics[width=0.47\columnwidth]{fetch-tree.pdf}}
        \qquad
        \subfloat[When multiple branches of the tree needs reading/updating, tree traversal can be done in parallel]{\label{fig:parallel-traversal}\includegraphics[width=0.47\columnwidth]{parallel-traversal.pdf}}
	\end{center}
    \caption{ZeroDB-specific optimizations of working with B-Trees. Allows to pre-fetch a tree or find multiple values in number of steps equal to the tree height}
	\label{fig:tree-traversal-optimizations}
\end{figure}

\subsubsection{Bulk-fetching small (sub)trees}
\label{sec:bulk-fetching}

When a tree (or sub-tree) is small enough, it could be more performant to fetch the entire tree to the client.
We do not need to do that bucket-by-bucket, as one would do with an index stored on a local hard drive, but in a logarithmic number of steps.

In order to do that, we fetch the root first~\figref{fig:fetch-tree}.
The client unpacks the root bucket and fetches all its children in one query.
Then it unpacks all the child nodes and learns the IDs of their children.
This process continues until the entire B-Tree is fetched.
Thus, the number of queries is equal to the height of the B-Tree, $H$, proportional to the logarithm of its size, $\log{S_{\mbox{ix}}}$.

When we fetch a tree, we implicitly show the number of objects it contains, $S_{\mbox{ix}}$, to the server.
Based on the size of the read data, the observer would be able to infer a number close to $S_{\mbox{ix}}$ and associate it with the bucket IDs just read.
Therefore, this technique should be used as rarely as possible, combined with reading other data at the same time or with oblivious RAMs~\cite{path-oram,burst-oram,oram-multicloud,ods-wang-2014}.
The latter would prevent the observer from learning bucket IDs.

\subsubsection{Parallel tree traversal}
\label{sec:parallel-traversal}

Often queries involve several values of the same field (or different fields).
For example, indexing a new document containing $100$ words could be an expensive operation since it would involve $100\,H$ requests to the server, making the performance defined by client-server latency very poor.

In order to fix that, we traverse the B-Tree in parallel~\figref{fig:parallel-traversal}.
We first fetch the root of the tree.
Then, we fetch only those child-buckets of the root which are relevant to the values in our query.
Then, we fetch only the relevant children of those, etc.
This way, we do tree traversal for all the necessary values in $H$ steps.

When we do parallel traversal, the server would be able to see access patterns but it would not be able to distinguish access patterns belonging to each of the values individually.

\section{Sharing data}

If there are multiple users of the same encrypted dataset, there are serious shortcomings with sharing the encryption key with all parties.
Since the architecture of ZeroDB allows using any encryption algorithm, we can use proxy re-encryption~\cite{afgh,libert2011unidirectional}
or delta-keys~\cite{delta-keys,mylar} to share data with other users (third parties) of the same dataset.
These algorithms allow the third party to hold an encryption key other than that used to encrypt the data,
while still being able to do the same queries as the data owner, until the server revokes their key.
We can also granularly share subsets of data, by combining these sharing algorithms with ZeroDB's query functionality.

Sharing an entire dataset is done as follows.
Suppose user $A$ owns the dataset and wants to share it with user $B$.
In the case of proxy re-encryption algorithms~\cite{afgh,libert2011unidirectional}, user $A$ has a key pair $priv_a/pub_a$ and user $B$ has a key pair $priv_b/pub_b$.
All objects and buckets are encrypted with random content-encryption keys $cek_i$ (individual for each object) using a block cipher algorithm (AES256/GCM by default).
The $cek$ is encrypted with key encrypting key $priv_a$.
So, we store $e_a(cek_i) = \mbox{encrypt}(priv_a, cek_i)$ and $c_i(obj) = \mbox{encrypt}(cek_i, data)$.

When user $A$ wants to give user $B$ access to the entire dataset and indexes, he constructs a re-encryption key $r_{ab}$ such that
the server can transform content-encryption key as $e_b(cek_i) = \mbox{transform}(r_{ab}, e_a(cek_i))$.
Re-encryption key $r_{ab}$ is given to the server, along with a revocation time.
While the server has this re-encryption key, it transforms CEK's of encrypted objects $e_a(cek_i) \rightarrow e_b(cek_i)$ as client $B$ requests the objects.
The server can revoke the key by removing $r_{ab}$ and by throttling queries.
Client $B$ is now able to do tree traversal, in the same manner as client $A$.

\subsection{Granular access permissions}

Let us first exemplify granular sharing of data in a scheme where $A$ shows encryption keys to $B$ and $A$ shares $M$ objects selected by a range query.
It is possible to share data granularly in such a way that $A$ has to only send $B$ number of keys proportional to $\log{M}$ rather than $M$.

In order to do that, keys $k^a$ must form a tree hierarchy similar to the one we had for set intersections~(\figref{fig:keytree}).
Client $A$ traverses the tree along the beginning and end of the range.
It shares keys for those buckets with $B$ giving $B$ the ability to do tree traversal within this range (buckets with blue background in~\figref{fig:keytree}).
However, there are buckets which are partly within the range.
Client $A$ passes key-reference pairs for buckets which belong to the contour of the range (there should be no more than $2\,s_b$ such pairs where $s_b$ is maximum bucket size).
The set of keys giving access to the in-range fraction of the index and the in-range contents of the buckets from contour of the range are given to $B$, encrypted with his public key.

The described sharing mechanism suffers from the fact that $B$ is given all information to decrypt objects within the range at once.
In order to make this access revokable, we need to use proxy re-encryption.
However, client $A$ should give hierarchical rights to re-encrypt the data to the server rather than to client $B$.
E.g. the server can only re-encrypt objects within the range given a set of re-encryption keys $r_{ab}$.
It is possible to do this using a conditional proxy re-encryption scheme (C-PRE)~\cite{conditional-pre-2009,unidirectional-pre-2010,conditional-pre-2014,hierarchical-pre}.

Usually such proxy re-encryption schemes support giving a product of multiple equality conditions.
The conditions which we apply would be object IDs in the bucket hierarchy, starting from the top.
For example, C-PRE encryption of a bucket in the second layer of the tree, $b_1$, would give the server ability to derive re-encryption keys for all of its children $b_{1*}$.
This gives the server ability to re-encrypt all buckets within the range specified by $A$, yet only requires $A$ to calculate a number of keys proportional to the $\log$ of the number of records shared.

\section{Security analysis}

ZeroDB is agnostic to the encryption algorithm used.
By default, we use authenticated strong encryption ({AES-256} in GCM mode).
Nothing is decrypted on the server unless we choose to use server-side set intersections~(Sec.~\ref{sec:server-intersection}).

Using deterministic or order-preserving encryption visible to the server could be detrimental to the security of the database in certain cases~\cite{cryptdb-hacked}.
However, even traversal of B-Trees leaves the possibility for an attacker to collect access statistics and infer the data in the database~\cite{access-pattern-attack}.
We provide a security analysis and an example of the number of queries necessary for such an attack.

\subsection{Threat model}

We adopt a threat model similar to the one used for the security analysis of CipherBase~\cite{cipherbase}.
We introduce a \emph{strong adversary}, who has unlimited observational power and can view the contents of memory and disk at every moment of time.
A \emph{weak adversary} can observe only a snapshot of memory and disk at one particular moment of time.
We assume that both strong and weak adversaries are passive (honest but curious) and do not return wrong results to the client.

\subsection{Data confidentiality}

\begin{figure}
	\begin{center}
        \includegraphics[width=0.7\columnwidth]{surname-attack.pdf}
	\end{center}
    \caption{Fraction of queries where last name can be inferred by a passive observer depending on the number of queries made to the database}
	\label{fig:surname-attack}
\end{figure}

In ZeroDB, all contents of the database and indexes are stored as encrypted objects.
Therefore, a weak adversary does not learn anything except for database size (including the number of fields, etc), even during queries.
So, ZeroDB provides \emph{semantic security} against a weak adversary.

A strong adversary, on the other hand, is able to observe access patterns and collect statistics of those over time.
So, we provide \emph{operational security} against a strong adversary.

First, we consider search queries which just require tree traversal.
For example, limit orders for queries which do not involve pulling a large subset of the index to the client.
For simplicity, let's assume that all we have in a database index is a B-Tree of customer surnames.
An active adversary is able to acquire access statistics of the index leaf nodes when searches of, say, $10$ people with a given last name are performed.
However, the amount of statistics should be large enough to be able to distinguish different records in the database.
If we assume that probability of a given surname appearing in a given query is the same as in the U.S. Census 2000 data~\cite{us-census-surnames}
and that keywords are distributed across queries according to a Poisson distribution,
an adversary needs to observe two keywords at least $N_1$ and $N_2$ times such that $|N_1 - N_2| > \sqrt{N_1 + N_2}$.
The fraction of last names which can be revealed by a strong adversary from a query given some number of queries is shown on~\figref{fig:surname-attack}.
For example, the surnames in $10\%$ of new queries can be inferred by a strong adversary after observing a million queries (although surnames not touched by queries are not revealed).
Unlike CipherBase~\cite{cipherbase}, our approach does not leak information about order of the elements.
By observing access patterns, the server can only figure out which buckets are leaf nodes and which are branch nodes.

In cases where we need to prefetch a large chunk of a B-Tree~(Sec.~\ref{sec:prefetch}), we immediately reveal information about the size of the sub-dataset to the server.
Of course, this is not desirable.
To avoid this, we can download adjacent records in the same query so that the downloaded size is always approximately constant and equal to that of the largest sub-dataset.
This way our security guarantees are similar to those for tree traversal.
In order for this to work we need to unfold nested trees into one tree~(Sec.~\ref{sec:unfold-trees}).

The security guarantees of server-side set intersections~(Sec.~\ref{sec:server-intersection}) are weaker.
One query leaks the sum of the sizes of the datasets involved in the query and how many elements satisfy any $c$ conditions from the query where $1\le c\le n$, where $n$~ is the number of conditions in the query.
The server can also assume that objects themselves (which are not stored in the index) will be read after the client obtains the result of the query,
those can be linked to deterministically encrypted object IDs.
``Fake'' deterministically encrypted object IDs can help to not open numbers of elements in sub-datasets but this method still shows the equality graph to the server.

We can significantly improve security guarantees by using ORAM techniques.
They allow us to hide access patterns by writing back data after reading it.
Several algorithms are available, though the most promising ones are Burst ORAM~\cite{burst-oram}
and oblivious data structures~\cite{ods-wang-2014}.
Still, these require writing back to the database about 20 times the amount of data read
(although these numbers improve every year).
It is also possible that hiding access patterns in a multi-cloud or fully decentralized arrangement has quite good performance properties~\cite{oram-multicloud},
which is especially interesting with decentralized storage layers such as Storj.io~\cite{storj} and IPFS~\cite{ipfs}.

\section{Performance}

\subsection{Optimizing bucket size}


ZeroDB requires several interactions between client and server in order to return query results.
Thus, both connection bandwidth $b$ and time of round-trip between client and server $\tau$, determine the performance of queries.
Moreover, there is an optimal bucket size $s_b$ for any given parameters.

Suppose we have an index in the form of a B-Tree of size $s_i$ bytes.
The size of one record in the bucket is $s_r$ bytes.
Apart from encryption one can apply compression.
The compression rate is defined as the ratio of initial bucket size to final (compressed + encrypted) bucket size is $c$.
The number of requests necessary to perform a query with an empty cache is equal to the height of B-Tree $h$, though we will also consider the case when we are allowed to cache up to $s_c$ bytes.


A good approximation for B-Tree height would is~\cite{wiki:b-tree}:
$$h = \log_m n,$$
where $m = s_b/s_r$ is the number of records in one bucket,
$n = s_i/s_r$ is the total number of records in the index,
assuming the best and the worst case heights are close to each other.
Thus:
$$h \approx \frac{\ln s_i - \ln s_r}{\ln s_b - \ln s_r}.$$

When we are allowed to use caching, we cache predominantly the top of the B-Tree with an average cached height approximately equal to the height of a B-Tree with same number of elements as the number of cached elements:
$$h_c \approx \frac{\ln s_c - \ln s_r}{\ln s_b - \ln s_r}.$$
The average number of interactions between client and server per query will be $k = h - h_c$.
Thus, the time which this interaction takes is:
$$\Delta t = k\tau + k\frac{s_b}{cb} =
\left(h - h_c \right) \left( \tau + \frac{s_b}{cb} \right) =
\frac{\ln s_i - \ln s_c}{\ln s_b - \ln s_r} \left( \tau + \frac{s_b}{cb} \right).$$

We seek to make queries as fast as possible (i.e. to minimize time $\Delta t$ necessary for a query to complete) seeking for optimal bucket size $s_b$.
The largest possible bucket size is equal to the size of the index and is obviously not efficient for sufficiently large indexes (it takes too long to download the index).
On the other hand, when bucket size is too small, the number of requests is too large, and query performance suffers due to latency between client and server.
An optimal compromise can be determined by equalizing first derivative to zero:
$$\frac{\partial \Delta t}{\partial s_b} = 0$$
which yields optimal bucket size:
$$s_b = \frac{cb\tau}{W\left( \frac{cb\tau}{e s_r}\right)},$$
where $W$ is real part of Lambert W function~\cite{wiki:lambert}
and $e=2.718281828\ldots$.

In ZeroDB, the typical size of a record in a branch bucket is $s_r = 29$ bytes,
compression rate with zlib $c=3$.
For a typical network bandwidth of $b=10^6$ (1~MB/s) and roundtrip time $\tau=0.05~\mbox{s}$, the optimal bucket size is $s_b=10$~kB when compression is not used, or $8$~kB when compression is used.
For a typical cache size of $5$~MB and index size of $50$~GB,
one query using this index on average takes $t=.07$~s and client-server exchange of $11$~kB of compressed data (without caching that would take $0.17$~s and exchanging $24$~kB of compressed data), reducing it down to an average of $1.3$ roundtrips.
We can finetune the optimal bucket size on the fly as the connection speed improves over time.

When we need to traverse several indexes at once, we bunch requests for several indexes in one query.
For example, first request for indexes $A$ and $B$ would be ``read root of indexes $A$ and $B$ (by object IDs of the roots)''.
That would make some of heavier queries impractical in the past but now they become practical due to internet bandwidth growing according to Nielsen's law~\cite{nielsen-law}.

\subsection{Unfolding trees with subtrees into one tree}
\label{sec:unfold-trees}

By default, we often use nested trees B-Trees of TreeSets.
While it's nice for performance reasons for traditional databases, it could be not so good when working with large latencies $\tau$,
or, more importantly, for security reasons.
For example, one cannot easily read adjancent records to hide the size of the dataset from the server.

Consider a B-Tree linking word IDs (wid) with TreeSets of object IDs (oid).
We can instead use a treeset of tuples (wid,~oid) which will be unique.
The tree will have approximately the same height for all wids.
Also it is easy to pre-set approximate size of the dataset to download if we don't want to show how many oids we have per wid.

\bibliography{zerodb}

\end{document}

